{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Managed vs External Tables in Spark\n",
    "\n",
    "This notebook explains **Managed vs External Tables in Spark** using an example dataset (`customers_1mb.csv`). It also demonstrates how to configure the **default warehouse path** and verify tables using **Hive Metastore**.\n",
    "\n",
    "## üîπ Key Concepts\n",
    "- **Managed Table**: Spark fully controls the **data and metadata**.\n",
    "- **External Table**: Spark manages only **metadata**, while the data remains **outside** Spark's control.\n",
    "- **Hive Metastore**: Stores **table definitions** to enable SQL-like querying in Spark.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 1: Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ‚úÖ Step 2: Define warehouse directory\n",
    "warehouse_location = '/tmp/pandu/warehouse'\n",
    "\n",
    "# ‚úÖ Step 3: Create Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('ManagedVsExternalTables') \\\n",
    "    .config('spark.sql.warehouse.dir', warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session created with warehouse location: {warehouse_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Checking Current Warehouse Directory\n",
    "You can verify the current **Spark SQL warehouse directory** using the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/spark-warehouse'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the configured warehouse directory\n",
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Loading the Dataset\n",
    "Now, we load the **customers_1mb.csv** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- registration_date: timestamp (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load CSV data into a DataFrame\n",
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header', 'True') \\\n",
    "    .option('inferSchema', 'True') \\\n",
    "    .load('/tmp/customers_1mb.csv')\n",
    "\n",
    "# Display DataFrame schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Creating a Temporary View\n",
    "We'll create a **temporary view** to allow SQL-like queries before creating tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "|customer_id|      name|     city|      state|country|  registration_date|is_active|\n",
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n",
      "|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n",
      "|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n",
      "|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n",
      "|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n",
      "+-----------+----------+---------+-----------+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view for querying\n",
    "df.createOrReplaceTempView(\"temp_customers\")\n",
    "\n",
    "# Query the view\n",
    "spark.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèó Creating a **Managed Table**\n",
    "- Spark **stores** the data inside the **warehouse directory** (`/tmp/pandu/warehouse`).\n",
    "- If you **drop** this table, the **data is also deleted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Managed Table\n",
    "spark.sql(\"DROP TABLE IF EXISTS managed_customers\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE managed_customers AS \n",
    "    SELECT * FROM temp_customers\n",
    "\"\"\")\n",
    "print(\"‚úÖ Managed table 'managed_customers' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER DATABASE default SET LOCATION 'file:/tmp/pandu/warehouse'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                |comment|\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|customer_id                 |int                                                      |null   |\n",
      "|name                        |string                                                   |null   |\n",
      "|city                        |string                                                   |null   |\n",
      "|state                       |string                                                   |null   |\n",
      "|country                     |string                                                   |null   |\n",
      "|registration_date           |timestamp                                                |null   |\n",
      "|is_active                   |boolean                                                  |null   |\n",
      "|                            |                                                         |       |\n",
      "|# Detailed Table Information|                                                         |       |\n",
      "|Database                    |default                                                  |       |\n",
      "|Table                       |managed_customers                                        |       |\n",
      "|Owner                       |root                                                     |       |\n",
      "|Created Time                |Sat Feb 01 17:20:05 UTC 2025                             |       |\n",
      "|Last Access                 |UNKNOWN                                                  |       |\n",
      "|Created By                  |Spark 3.3.2                                              |       |\n",
      "|Type                        |MANAGED                                                  |       |\n",
      "|Provider                    |hive                                                     |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1738430407]                       |       |\n",
      "|Statistics                  |1219563 bytes                                            |       |\n",
      "|Location                    |hdfs://my-cluster-m/user/hive/warehouse/managed_customers|       |\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended managed_customers').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Verify Managed Table\n",
    "Run the following command in a **Dataproc terminal** to check where the managed table is stored:\n",
    "```bash\n",
    "!hdfs dfs -ls /tmp/pandu/warehouse/managed_customers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Creating an **External Table**\n",
    "- The data **remains** in `/tmp/customers_1mb.csv`.\n",
    "- If you **drop** this table, the **data is not deleted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an External Table\n",
    "spark.sql(\"DROP TABLE IF EXISTS external_customers\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE external_customers \n",
    "    USING CSV \n",
    "    LOCATION '/tmp/customers_1mb.csv'\n",
    "\"\"\")\n",
    "print(\"‚úÖ External table 'external_customers' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Verify External Table\n",
    "Run the following command to check its location:\n",
    "```bash\n",
    "!hdfs dfs -ls /tmp/customers_1mb.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Verifying Tables in Hive Metastore\n",
    "You can check all available tables in Spark using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tables in Spark\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõë Dropping the Tables\n",
    "Dropping a **Managed Table** deletes both **metadata and data**, while dropping an **External Table** deletes only **metadata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop managed table (Data is deleted!)\n",
    "spark.sql(\"DROP TABLE IF EXISTS managed_customers\")\n",
    "\n",
    "# Drop external table (Data is NOT deleted!)\n",
    "spark.sql(\"DROP TABLE IF EXISTS external_customers\")\n",
    "\n",
    "print(\"‚úÖ Tables dropped successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "| Feature | Managed Table | External Table |\n",
    "|---------|--------------|----------------|\n",
    "| Data Location | Inside warehouse | Custom location |\n",
    "| Dropping Table | Deletes data | Only deletes metadata |\n",
    "| Performance | Optimized by Spark | Depends on external storage |\n",
    "\n",
    "**üöÄ Now you understand the difference between Managed and External Tables in Spark!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
